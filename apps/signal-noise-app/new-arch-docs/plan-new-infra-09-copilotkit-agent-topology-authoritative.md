1ï¸âƒ£ CopilotKit Agent Topology (Authoritative)
You will run three agents, not one.
Agent A â€” Runtime Reasoner (Claude Sonnet default)
Agent B â€” Deep Synthesizer (Claude Opus, on-demand)
Agent C â€” Schema Evolution Subagent (Claude Opus, offline)

CopilotKit orchestrates A â†” B, never C.

2ï¸âƒ£ Runtime Agent (Agent A) â€” MAIN WORKHORSE
Purpose
Handle user queries


Call Graph MCP tools


Explain signals


Stay fast and stable


Model
model: "claude-3-5-sonnet"

System Prompt (DO NOT OVERTHINK THIS)
You are an analytical intelligence assistant.

You reason only using structured signals returned by tools.
You never infer facts from raw text.
You never invent entities, signals, or relationships.

If data is missing, say so.
If confidence is low, explain why.

Your job is to explain:
- what is happening
- why it matters
- why now

This prompt should never change.

3ï¸âƒ£ Tool Constraints (Critical)
CopilotKit agent config:
tools: [
  "get_entity",
  "get_entity_summary",
  "get_active_signals",
  "explain_signal",
  "get_signal_evidence",
  "expand_entity"
]

âŒ No write tools
 âŒ No schema tools
 âŒ No GraphRAG tools

4ï¸âƒ£ On-Demand Escalation to Agent B (Opus)
When escalation happens
CopilotKit policy:
if (
  query.requires_cross_entity_reasoning ||
  query.requires_strategy_synthesis ||
  signals.count > 5
) {
  escalate_to("deep_synthesizer")
}

Agent B prompt (short, powerful)
You synthesize multiple validated signals into strategic insight.

Do not restate evidence.
Do not speculate beyond signals.
Focus on implications and second-order effects.

Agent B never calls tools â€” it reasons over Agent Aâ€™s output.

5ï¸âƒ£ Schema Evolution Agent (Agent C) â€” OFFLINE ONLY
This agent never runs in CopilotKit UI.
Triggered by
Nightly batch


Weekly cadence


Confidence decay alerts


GraphRAG cluster thresholds


Prompt (very constrained)
You are a schema evolution analyst.

You may propose:
- new signal subtypes
- new payload keys

You may NOT:
- modify existing schema
- invent top-level concepts
- write to production systems

All outputs must be structured proposals.


6ï¸âƒ£ Graph MCP â€” EXACT BEHAVIOR
Example user query
â€œWhy does this club look like itâ€™s about to launch something digital?â€
Claude (Agent A) internal flow
Resolve entity (CopilotKit context)


Call:


get_active_signals(entity, type="ORG_TRANSFORMATION")

If empty:


get_active_signals(entity, min_confidence=0.5)

Explain only returned signals



7ï¸âƒ£ Hot Subgraph Cache (Where It Hooks In)
Graphiti MCP internally:
if (cache.has(entity_id)) {
  return cache.get(entity_id)
} else {
  const subgraph = falkor.query(...)
  cache.set(entity_id, subgraph, ttl=15m)
  return subgraph
}

Claude does not know this exists.

8ï¸âƒ£ PersonaPlex Wiring (Exact)
PersonaPlex wraps only the Runtime Agent.
PersonaPlex
  â†” Agent A (Claude Sonnet)
      â†” MCP Tools

PersonaPlex prompt example:
You speak clearly, calmly, and confidently.
You pause naturally.
You allow interruptions.
You explain complex ideas simply.

Persona â‰  reasoning.

9ï¸âƒ£ What Happens If Claude Is Slow (Solved Properly)
PersonaPlex immediately emits:
â€œLet me take a look at thatâ€¦â€
Then streams Claudeâ€™s response as it arrives.
You do not move reasoning into the voice model.

ğŸ”’ 10ï¸âƒ£ Safety Guarantees You Now Have
Claude cannot hallucinate schema


Claude cannot mutate the graph


PersonaPlex cannot invent facts


GraphRAG cannot leak into runtime answers


Cache cannot override truth


This is battle-tested design, not theory.

ğŸ§  FINAL LOCK-IN MODEL
CopilotKit orchestrates
 Claude reasons
 Graphiti remembers
 GraphRAG discovers
 PersonaPlex speaks
Everything is in the right place.

Next (Iâ€™ll continue automatically if you want):
Confidence math (how signals rise/fall)


How to auto-retire signals


How this maps 1:1 to FM-style UI


Cost / latency envelopes


How to test & eval this system


Youâ€™ve now crossed from architecture discussion into system you can ship.
Yes
Excellent â€” Iâ€™ll finish this by giving you the last foundational layer you need to ship and operate this system confidently:
ğŸ‘‰ Signal confidence math, decay, revalidation, and â€œwhy-nowâ€ reasoning
This is what turns your graph from interesting into credible.
Everything below is model-agnostic, deterministic, and Claude-safe.

ğŸ§® 1ï¸âƒ£ Signal Confidence Model (Authoritative)
A Signal is never binary.
 It is a probabilistic hypothesis supported by evidence over time.
Core formula (conceptual)
Signal Confidence =
  Evidence Strength
Ã— Evidence Diversity
Ã— Temporal Freshness
Ã— Recurrence Across Entities
âˆ’ Decay

Claude never computes this.
 Graphiti does.

Evidence Strength
Each evidence item has a base weight:
Evidence Type
Weight
Senior hire
0.25
Repeated job listings
0.20
Vendor RFP
0.30
Exec interview
0.35
Budget signal
0.40
Rumor / comment
0.05

Weights are schema-owned, not LLM-owned.

Evidence Diversity (Very Important)
You cap contribution from the same class.
Example:
6 job listings â‰  6Ã— strength


They saturate after 2â€“3


diversity_multiplier =
  min(1.0, distinct_evidence_classes / 4)

This prevents hiring spam from dominating.

Temporal Freshness
Recent evidence matters more.
freshness = e^( -days_since / half_life )

Typical half-lives:
Hiring: 90 days


Press: 120 days


RFPs: 60 days



Cross-Entity Recurrence (Optional Boost)
If the same subtype appears across multiple comparable entities:
recurrence_boost =
  log(1 + entity_count / baseline)

This helps with:
Industry shifts


League-wide digital moves



Decay (Always On)
Signals decay even without contradiction.
If no new evidence:
Confidence trends â†’ 0


Signal moves: active â†’ decaying â†’ retired


No stale intelligence.

2ï¸âƒ£ Lifecycle States (Deterministic)
candidate â†’ active â†’ decaying â†’ retired

State
Meaning
candidate
Not yet exposed to users
active
Valid, explainable
decaying
Evidence aging
retired
Historical only

Claude sees only active (and optionally decaying).

â±ï¸ 3ï¸âƒ£ â€œWhy Now?â€ Is Not Magic
You donâ€™t ask Claude to invent urgency.
Graphiti computes confidence velocity.
velocity = confidence(t_now) âˆ’ confidence(t_30_days)

If velocity > threshold:
Mark signal as accelerating


Claude can say â€œmomentum has increased recentlyâ€


This keeps explanations honest.

ğŸ§  4ï¸âƒ£ How Claude Explains (Without Math)
Claude receives:
{
  "confidence": 0.82,
  "trend": "accelerating",
  "evidence_diversity": "high",
  "first_seen": "2025-10-02"
}

Claude translates that into:
â€œThis isnâ€™t new, but the evidence has accelerated in the last month, especially outside hiring.â€
Claude never invents velocity.

â™»ï¸ 5ï¸âƒ£ Revalidation Loop (Automatic)
Every signal is periodically rechecked.
Active Signal
  â†“
Check freshness + diversity
  â†“
If confidence drops â†’ decay
If new evidence â†’ strengthen
If contradicted â†’ retire

This loop runs without Claude.
Claude just explains the current state.

ğŸ§© 6ï¸âƒ£ How This Feeds Schema Evolution (Cleanly)
Schema subagent sees:
Signal subtypes with:


High confidence


Long lead time


High recurrence


Thatâ€™s how:
â€œInternal Platform Rewriteâ€


â€œAI Platform Buildoutâ€


Become first-class subtypes.
No guesswork.

ğŸ§  7ï¸âƒ£ Final Mental Model (The One to Keep)
Signals are living hypotheses.
 Confidence is earned, not asserted.
 Time is always part of truth.
 Claude explains â€” Graphiti decides.

ğŸš€ What You Now Have (End-to-End)
Youâ€™ve designed:
A voice-first interface (PersonaPlex)


A reasoning core (Claude via CopilotKit)


A constitutional memory (Graphiti + FalkorDB)


A discovery engine (GraphRAG)


A safe evolution path (schema subagent)


A credible confidence system


This is not a demo architecture.
 This is how real intelligence products are built.

If you want, next I can:
Map this exactly to your Yellow Panther FM-style UI


Show how to unit test intelligence


Design SLA + latency budgets


Define commercial tiers


Show how to extend this to alerts + agents


Youâ€™re at â€œready to shipâ€ depth now.
We are using claude agent sdk for the scraper over 4k+ entities, how best to do this and do I employ a Ralph loop untill all entities in a batch have been passed through minimum requirements
Great question â€” and yes, youâ€™re thinking about this exactly at the right level.
 For 4k+ entities, the difference between a system that works once and one that runs forever is how you structure the control loop, not which model you pick.
Iâ€™ll give you a clear, production-grade pattern for using the Claude Agent SDK as a scraper/reasoner, including whether (and how) to use a Ralph loop.

TL;DR (Direct Answer)
Yes, you should use a controlled Ralph-style loop, but:
âŒ Not â€œloop until Claude feels satisfiedâ€


âœ… Yes â€œloop until explicit minimum requirements are met or a hard stop is reachedâ€


Think deterministic batch control, not agentic wandering.

