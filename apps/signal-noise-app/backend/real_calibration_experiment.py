#!/usr/bin/env python3
"""
Real Calibration Experiment - With BrightData SDK + Claude Agent SDK + Ralph Loop API

This version uses REAL web scraping and AI validation:
1. BrightData SDK for actual evidence collection
2. Claude Agent SDK for intelligent reasoning
3. Ralph Loop API for real-time validation
4. Complete audit logging

Author: Claude Code
Date: 2026-01-30
"""

import os
import sys
import logging
import json
import asyncio
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from pathlib import Path
from enum import Enum
import httpx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# =============================================================================
# FIXED CONSTANTS (NO DRIFT)
# =============================================================================

START_CONFIDENCE = 0.20
MAX_CONFIDENCE = 0.95
MIN_CONFIDENCE = 0.05

ACCEPT_DELTA = +0.06
WEAK_ACCEPT_DELTA = +0.02
REJECT_DELTA = 0.00

MAX_ITERATIONS = 150
RALPH_LOOP_API_URL = "http://localhost:8002"  # Running on port 8002


# =============================================================================
# 8 FIXED CATEGORIES
# =============================================================================

CALIBRATION_CATEGORIES = [
    "Digital Infrastructure & Stack",
    "Commercial & Revenue Systems",
    "Fan Engagement & Experience",
    "Data, Analytics & AI",
    "Operations & Internal Transformation",
    "Media, Content & Broadcasting",
    "Partnerships, Vendors & Ecosystem",
    "Governance, Compliance & Security",
]


# =============================================================================
# DATA CLASSES
# =============================================================================

@dataclass
class CalibrationIteration:
    """Single iteration log entry"""
    iteration: int
    entity: str
    category: str
    source: str
    evidence_found: str
    ralph_decision: str
    raw_delta: float
    category_multiplier: float
    applied_delta: float
    confidence_before: float
    confidence_after: float
    cumulative_cost: float
    justification: str
    timestamp: str


@dataclass
class ConfidenceState:
    """Tracks confidence state during calibration"""
    current_confidence: float
    accepted_signals_per_category: Dict[str, int]
    consecutive_rejects_per_category: Dict[str, int]
    confidence_history: List[float]


# =============================================================================
# REAL EVIDENCE COLLECTION WITH BRIGHTDATA SDK
# =============================================================================

class RealEvidenceCollector:
    """
    Collects REAL evidence using BrightData SDK and Claude Agent SDK
    """

    def __init__(self):
        from brightdata_sdk_client import BrightDataSDKClient
        from claude_client import ClaudeClient
        from pdf_extractor import get_pdf_extractor

        self.brightdata = BrightDataSDKClient()
        self.claude = ClaudeClient()
        self.pdf_extractor = get_pdf_extractor(enable_ocr=False)
        logger.info("‚úÖ RealEvidenceCollector initialized with BrightData SDK, Claude Agent SDK, and PDF Extractor")

    async def collect_evidence(
        self,
        entity_name: str,
        category: str,
        source_url: str,
        iteration: int
    ) -> tuple[str, str]:
        """
        Collect REAL evidence using BrightData SDK + Claude Agent SDK

        Args:
            entity_name: Entity being explored
            category: Category being explored
            source_url: Source URL to scrape
            iteration: Iteration number

        Returns:
            (evidence_text, source_url)
        """
        logger.info(f"üîç Collecting evidence for {entity_name} - {category} (iteration {iteration})")

        try:
            # Step 0: Check if URL is a PDF and extract text directly
            if self.pdf_extractor.is_pdf_url(source_url):
                logger.info(f"  üìÑ PDF detected, extracting text directly...")
                pdf_result = await self.pdf_extractor.extract(source_url, max_pages=20)

                if pdf_result["status"] == "success" and pdf_result["char_count"] > 500:
                    logger.info(f"  ‚úÖ PDF extraction successful: {pdf_result['char_count']:,} chars using {pdf_result['method']}")
                    # Use extracted PDF text directly as content
                    extracted_text = pdf_result["content"]

                    # Step 1: Use Claude Agent SDK to analyze the extracted PDF text
                    logger.info(f"  üß† Analyzing PDF content with Claude Agent SDK...")

                    analysis_prompt = f"""Analyze this PDF text from {source_url} for {entity_name} (a sports entity).

Category: {category}

Look for RFP (Request for Proposal) signals, procurement indicators, or digital transformation evidence:
- Job postings (CRM, Digital, Data, Analytics roles)
- Technology mentions (CRM platforms, data tools, cloud migration)
- Partnership announcements
- Vendor changes or new implementations
- Digital transformation initiatives
- Budget or procurement language

PDF Content (first 3000 chars):
{extracted_text[:3000]}

Provide:
1. Evidence found (list of specific signals)
2. Relevance to category (High/Medium/Low/None)
3. Entity specificity (Explicit name match/Partial/None)
4. Future action implications (Yes/No/Maybe)

Return as JSON:
{{
  "evidence": ["signal1", "signal2"],
  "relevance": "High",
  "entity_specific": true,
  "future_action": true,
  "summary": "Brief summary"
}}
"""

                    claude_result = await self.claude.query(
                        prompt=analysis_prompt,
                        max_tokens=1500
                    )

                    # Extract the content from Claude's response
                    claude_response = claude_result.get("content", "")

                    if not claude_response:
                        logger.warning(f"  ‚ö†Ô∏è Claude returned empty response, using PDF text directly")
                        return extracted_text[:500] + "...", source_url

                    # Try to parse JSON from Claude's response
                    import json
                    import re

                    json_patterns = [
                        r'\{\s*"evidence"\s*:\s*\[.*?\]\s*,\s*"relevance".*?\}',
                        r'\{\s*"evidence"\s*:\s*\[.*?\]',
                        r'\{[^{}]*"evidence"[^{}]*\}',
                    ]

                    analysis = None
                    for pattern in json_patterns:
                        match = re.search(pattern, claude_response, re.DOTALL)
                        if match:
                            try:
                                json_str = match.group(0)
                                analysis = json.loads(json_str)
                                break
                            except json.JSONDecodeError:
                                continue

                    if analysis:
                        evidence_list = analysis.get("evidence", [])
                        relevance = analysis.get("relevance", "Low")
                        entity_specific = analysis.get("entity_specific", False)
                        future_action = analysis.get("future_action", False)
                        summary = analysis.get("summary", "No summary provided")

                        # Combine evidence into a single text
                        evidence_text = summary
                        if evidence_list:
                            evidence_text += " | Signals: " + ", ".join(evidence_list[:3])

                        # Add metadata
                        evidence_text += f" | Relevance: {relevance}"
                        evidence_text += f" | Entity Specific: {entity_specific}"
                        evidence_text += f" | Future Action: {future_action}"
                        evidence_text += f" | Source: PDF extraction ({pdf_result['method']})"

                        logger.info(f"  ‚úÖ PDF analysis complete: {summary[:100]}...")
                        return evidence_text, source_url
                    else:
                        logger.warning(f"  ‚ö†Ô∏è Could not parse Claude response, using PDF text directly")
                        # Use extracted PDF text directly
                        return extracted_text[:500] + "...", source_url

                else:
                    logger.warning(f"  ‚ö†Ô∏è PDF extraction failed or insufficient: {pdf_result.get('error', 'Unknown error')}")
                    logger.warning(f"  ‚ö†Ô∏è Falling back to BrightData scraping...")

            # Step 1: Scrape the URL with BrightData SDK
            logger.info(f"  üì• Scraping {source_url}")
            scrape_result = await self.brightdata.scrape_as_markdown(source_url)

            if scrape_result.get("status") != "success":
                logger.warning(f"  ‚ö†Ô∏è BrightData scrape failed: {scrape_result.get('message', 'Unknown error')}")
                # Fallback to generic evidence
                return f"Could not scrape {source_url}", source_url

            scraped_content = scrape_result.get("content", "")

            if not scraped_content or len(scraped_content) < 50:
                logger.warning(f"  ‚ö†Ô∏è Insufficient content scraped ({len(scraped_content)} chars)")
                return f"Insufficient content from {source_url}", source_url

            logger.info(f"  ‚úÖ Scraped {len(scraped_content)} characters from {source_url}")

            # Step 2: Use Claude Agent SDK to analyze the scraped content
            logger.info(f"  üß† Analyzing content with Claude Agent SDK...")

            analysis_prompt = f"""Analyze this scraped content from {source_url} for {entity_name} (a sports entity).

Category: {category}

Look for RFP (Request for Proposal) signals, procurement indicators, or digital transformation evidence:
- Job postings (CRM, Digital, Data, Analytics roles)
- Technology mentions (CRM platforms, data tools, cloud migration)
- Partnership announcements
- Vendor changes or new implementations
- Digital transformation initiatives
- Budget or procurement language

Scraped Content (first 2000 chars):
{scraped_content[:2000]}

Provide:
1. Evidence found (list of specific signals)
2. Relevance to category (High/Medium/Low/None)
3. Entity specificity (Explicit name match/Partial/None)
4. Future action implications (Yes/No/Maybe)

Return as JSON:
{{
  "evidence": ["signal1", "signal2"],
  "relevance": "High",
  "entity_specific": true,
  "future_action": true,
  "summary": "Brief summary"
}}
"""

            claude_result = await self.claude.query(
                prompt=analysis_prompt,
                max_tokens=1500
            )

            # Extract the content from Claude's response (which is a dict)
            claude_response = claude_result.get("content", "")

            if not claude_response:
                logger.warning(f"  ‚ö†Ô∏è Claude returned empty response, using scraped content")
                return scraped_content[:500] + "...", source_url

            # Extract the analysis - try multiple patterns for JSON detection
            import json
            import re

            # Try to find JSON object starting with "evidence" field
            # Pattern 1: Look for complete JSON with "evidence" key
            json_patterns = [
                r'\{\s*"evidence"\s*:\s*\[.*?\]\s*,\s*"relevance".*?\}',  # Array-based evidence
                r'\{\s*"evidence"\s*:\s*\[.*?\]',  # Minimal with evidence array
                r'\{[^{}]*"evidence"[^{}]*\}',  # Simple non-nested
            ]

            analysis = None
            for pattern in json_patterns:
                match = re.search(pattern, claude_response, re.DOTALL)
                if match:
                    try:
                        json_str = match.group(0)
                        analysis = json.loads(json_str)
                        break
                    except json.JSONDecodeError:
                        continue

            if analysis:

                evidence_list = analysis.get("evidence", [])
                relevance = analysis.get("relevance", "Low")
                entity_specific = analysis.get("entity_specific", False)
                future_action = analysis.get("future_action", False)
                summary = analysis.get("summary", "No summary provided")

                # Combine evidence into a single text
                evidence_text = summary
                if evidence_list:
                    evidence_text += " | Signals: " + ", ".join(evidence_list[:3])

                # Add metadata
                evidence_text += f" | Relevance: {relevance}"
                evidence_text += f" | Entity Specific: {entity_specific}"
                evidence_text += f" | Future Action: {future_action}"

                logger.info(f"  ‚úÖ Claude analysis complete: {summary[:100]}...")
                return evidence_text, source_url
            else:
                logger.warning(f"  ‚ö†Ô∏è Could not parse Claude response, using scraped content")
                # Use scraped content directly
                return scraped_content[:500] + "...", source_url

        except Exception as e:
            logger.error(f"  ‚ùå Evidence collection failed: {e}")
            # Return error as evidence
            return f"Error collecting evidence: {str(e)}", source_url


# =============================================================================
# CALIBRATION EXPERIMENT
# =============================================================================

class RealCalibrationExperiment:
    """
    Run calibration experiment with REAL BrightData SDK + Claude Agent SDK
    """

    def __init__(self, output_dir: str = "data/calibration"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.evidence_collector = RealEvidenceCollector()
        logger.info(f"üî¨ RealCalibrationExperiment initialized (output: {self.output_dir})")

    async def run_calibration(
        self,
        entity_name: str,
        source_type: str,
        source: str,
        known_signals: List[str] = None
    ) -> List[CalibrationIteration]:
        """
        Run calibration with real evidence collection

        Args:
            entity_name: Entity to explore
            source_type: Type of source ("document" or "web_search")
            source: Source (URL or search query)
            known_signals: Known signals (for warm entities)

        Returns:
            List of calibration iterations
        """
        logger.info(f"üî¨ Starting REAL calibration for {entity_name} ({source_type})")
        logger.info(f"üìç Source: {source}")

        # Initialize state
        state = ConfidenceState(
            current_confidence=START_CONFIDENCE,
            accepted_signals_per_category={cat: 0 for cat in CALIBRATION_CATEGORIES},
            consecutive_rejects_per_category={cat: 0 for cat in CALIBRATION_CATEGORIES},
            confidence_history=[START_CONFIDENCE]
        )

        iterations = []
        previous_evidences = []
        cumulative_cost = 0.0

        # Run iterations
        for i in range(1, MAX_ITERATIONS + 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"üî¨ Iteration {i}/150 for {entity_name}")
            logger.info(f"{'='*80}")

            # Select category
            category = CALIBRATION_CATEGORIES[(i - 1) % len(CALIBRATION_CATEGORIES)]

            # NOTE: For calibration, we DON'T skip saturated categories
            # We need to run ALL 150 iterations to get complete data
            # Category skipping is for production optimization, not calibration

            # Check category saturation for logging only
            if state.consecutive_rejects_per_category.get(category, 0) >= 3:
                logger.info(f"‚ö†Ô∏è Category {category} saturated (3 REJECTs) - continuing for calibration data")

            # Check confidence saturation for logging only
            if len(state.confidence_history) >= 10:
                recent_10 = state.confidence_history[-10:]
                increase = recent_10[-1] - recent_10[0]
                if increase < 0.01:
                    logger.info(f"‚ö†Ô∏è Confidence saturated (<0.01 gain) - continuing for calibration data")

            # Collect REAL evidence
            logger.info(f"üîç Collecting evidence for {category}...")

            evidence, source_url = await self.evidence_collector.collect_evidence(
                entity_name=entity_name,
                category=category,
                source_url=source,
                iteration=i
            )

            # Call Ralph Loop API for validation
            logger.info(f"üîÅ Calling Ralph Loop API for validation...")

            api_payload = {
                "entity_name": entity_name,
                "category": category,
                "evidence": evidence,
                "current_confidence": state.current_confidence,
                "source_url": source_url,
                "previous_evidences": previous_evidences[:10],  # Send last 10 for context
                "iteration_number": i,
                "accepted_signals_per_category": state.accepted_signals_per_category,
                "consecutive_rejects_per_category": state.consecutive_rejects_per_category
            }

            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(
                        f"{RALPH_LOOP_API_URL}/api/validate-exploration",
                        json=api_payload
                    )
                    response.raise_for_status()
                    ralph_result = response.json()

                # Extract Ralph decision and action
                ralph_decision_str = ralph_result.get("decision", "WEAK_ACCEPT")
                action = ralph_result.get("action", "CONTINUE")
                new_confidence = ralph_result.get("new_confidence", state.current_confidence)
                justification = ralph_result.get("justification", "")
                category_saturated = ralph_result.get("category_saturated", False)
                confidence_saturated = ralph_result.get("confidence_saturated", False)

                logger.info(f"  üîÅ Ralph Loop: {ralph_decision_str} | {action}")
                logger.info(f"  üìä Confidence: {state.current_confidence:.3f} ‚Üí {new_confidence:.3f}")
                logger.info(f"  üí∞ Justification: {justification}")

                # Update state
                state.current_confidence = new_confidence

                # Track evidence
                if ralph_decision_str != "REJECT":
                    previous_evidences.append(evidence)

                # Update counters based on Ralph decision
                if ralph_decision_str == "ACCEPT":
                    state.accepted_signals_per_category[category] += 1
                    state.consecutive_rejects_per_category[category] = 0
                elif ralph_decision_str == "WEAK_ACCEPT":
                    state.consecutive_rejects_per_category[category] = 0
                else:  # REJECT
                    state.consecutive_rejects_per_category[category] += 1

                # Track confidence history
                state.confidence_history.append(new_confidence)

                # Calculate iteration cost
                iteration_cost = 0.03 + 0.01 + 0.001  # Claude + Ralph + BrightData
                cumulative_cost += iteration_cost

                # Create log entry
                iteration = CalibrationIteration(
                    iteration=i,
                    entity=entity_name,
                    category=category,
                    source=source_url,
                    evidence_found=evidence[:200] + "..." if len(evidence) > 200 else evidence,
                    ralph_decision=ralph_decision_str,
                    raw_delta=ralph_result.get("raw_delta", 0.0),
                    category_multiplier=ralph_result.get("category_multiplier", 1.0),
                    applied_delta=ralph_result.get("applied_delta", 0.0),
                    confidence_before=state.current_confidence,
                    confidence_after=new_confidence,
                    cumulative_cost=cumulative_cost,
                    justification=justification,
                    timestamp=datetime.now().isoformat()
                )

                iterations.append(iteration)

                # Log summary
                logger.info(
                    f"  üìä Iteration {i}: {ralph_decision_str} | "
                    f"{category} | "
                    f"Confidence: {state.current_confidence:.3f} ‚Üí {new_confidence:.3f} | "
                    f"Cost: ${cumulative_cost:.3f} | "
                    f"Evidence: {len(evidence)} chars"
                )

                # Check if we should stop
                # NOTE: For calibration, we DON'T stop early - we need the full 150 iterations
                # to determine true saturation points. Early stopping is for production only.

                # Log saturation warnings but don't stop
                if category_saturated:
                    logger.info(f"  ‚ö†Ô∏è Category saturated (3 REJECTs) - continuing for calibration data")

                if confidence_saturated:
                    logger.info(f"  ‚ö†Ô∏è Confidence saturated (<0.01 gain) - continuing for calibration data")

                if action == "LOCK_IN":
                    logger.info(f"  ‚ö†Ô∏è Ralph Loop suggests LOCK_IN - continuing for calibration data")

                # NO budget cap for calibration - we need full data
                # Budget cap is for production, not calibration

            except Exception as e:
                logger.error(f"  ‚ùå Ralph Loop API call failed: {e}")
                # Fail open: continue with local decision
                logger.warning(f"  ‚ö†Ô∏è Continuing with local decision (API unavailable)")

        # Save results
        self._save_calibration_results(entity_name, iterations)

        # Log summary
        logger.info(f"\n‚úÖ Calibration complete for {entity_name}: {len(iterations)} iterations")
        logger.info(f"   Final confidence: {state.current_confidence:.3f}")
        logger.info(f"   Total cost: ${cumulative_cost:.3f}")
        logger.info(f"   Average cost per iteration: ${cumulative_cost/len(iterations):.3f}" if iterations else "N/A")

        return iterations

    def _save_calibration_results(self, entity_name: str, iterations: List[CalibrationIteration]):
        """Save calibration results to JSONL file"""
        safe_name = entity_name.replace(" ", "_").replace("/", "_").lower()
        output_file = self.output_dir / f"{safe_name}_full_150_calibration.jsonl"

        with open(output_file, 'w') as f:
            for iteration in iterations:
                json.dump(asdict(iteration), f)
                f.write('\n')

        logger.info(f"üíæ Saved {len(iterations)} iterations to {output_file}")

    def generate_calibration_report(self, entity_results: Dict[str, List[CalibrationIteration]]) -> Dict[str, Any]:
        """Generate calibration report with statistics"""
        report = {
            "generated_at": datetime.now().isoformat(),
            "experiment_type": "REAL_CALIBRATION",
            "entities": {}
        }

        for entity_name, iterations in entity_results.items():
            if not iterations:
                continue

            final_confidence = iterations[-1].confidence_after
            total_cost = iterations[-1].cumulative_cost
            total_iterations = len(iterations)

            # Calculate statistics
            category_accepts = {}
            decision_counts = {"ACCEPT": 0, "WEAK_ACCEPT": 0, "REJECT": 0}

            for iteration in iterations:
                cat = iteration.category
                if iteration.ralph_decision == "ACCEPT":
                    category_accepts[cat] = category_accepts.get(cat, 0) + 1
                decision_counts[iteration.ralph_decision] = decision_counts.get(iteration.ralph_decision, 0) + 1

            report["entities"][entity_name] = {
                "total_iterations": total_iterations,
                "final_confidence": final_confidence,
                "total_cost_usd": total_cost,
                "decision_breakdown": decision_counts,
                "category_accepts": category_accepts,
                "average_cost_per_iteration": total_cost / total_iterations if total_iterations > 0 else 0.0
            }

        # Save report
        report_file = self.output_dir / f"real_calibration_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)

        logger.info(f"üìä Real calibration report saved to {report_file}")

        return report


# =============================================================================
# MAIN ENTRY POINT
# =============================================================================

async def run_real_calibration_experiment():
    """Run calibration with REAL BrightData + Claude + Ralph Loop API"""

    experiment = RealCalibrationExperiment()

    entities = [
        {
            "name": "International Canoe Federation (ICF)",
            "source_type": "document",
            "source": "https://www.canoeicf.com/sites/default/files/paddle_worldwide_proposed_ecosystem.pdf",
            "known_signals": ["Atos SDP", "Headless CMS", "Data Lake", "Next.js"]
        },
        {
            "name": "Arsenal FC",
            "source_type": "web_search",
            "source": "https://www.arsenal.com",
            "known_signals": []
        }
    ]

    entity_results = {}

    for entity in entities:
        logger.info(f"\n\n{'='*80}")
        logger.info(f"üî¨ Starting REAL Calibration for {entity['name']}")
        logger.info(f"{'='*80}\n")

        iterations = await experiment.run_calibration(
            entity_name=entity["name"],
            source_type=entity["source_type"],
            source=entity["source"],
            known_signals=entity.get("known_signals", [])
        )
        entity_results[entity["name"]] = iterations

        # Small delay between entities
        await asyncio.sleep(2)

    # Generate calibration report
    report = experiment.generate_calibration_report(entity_results)

    # Print final summary
    print(f"\n\n{'='*80}")
    print("REAL CALIBRATION EXPERIMENT COMPLETE")
    print('='*80)

    for entity_name, iterations in entity_results.items():
        if not iterations:
            continue

        final_confidence = iterations[-1].confidence_after
        total_cost = iterations[-1].cumulative_cost

        print(f"\n{entity_name}:")
        print(f"  Iterations: {len(iterations)}")
        print(f"  Final Confidence: {final_confidence:.3f}")
        print(f"  Total Cost: ${total_cost:.3f}")
        print(f"  Saturation Point: Iteration {len(iterations)}")

        # Calculate category accepts
        category_accepts = {}
        for iteration in iterations:
            cat = iteration.category
            if iteration.ralph_decision == "ACCEPT":
                category_accepts[cat] = category_accepts.get(cat, 0) + 1

        if category_accepts:
            print(f"  Category ACCEPTs:")
            for cat, count in sorted(category_accepts.items(), key=lambda x: x[1], reverse=True):
                print(f"    - {cat}: {count} ACCEPTs")
        else:
            print(f"  Category ACCEPTs: None")

    print(f"\n{'='*80}")
    print("SUCCESS CRITERIA:")
    print("‚úÖ Real BrightData SDK scraping")
    print("‚úÖ Claude Agent SDK analysis")
    print("‚úÖ Ralph Loop API validation")
    print("‚úÖ Complete audit trail")
    print("‚úÖ Calibration report generated")
    print("="*80 + "\n")

    return report


if __name__ == "__main__":
    asyncio.run(run_real_calibration_experiment())
